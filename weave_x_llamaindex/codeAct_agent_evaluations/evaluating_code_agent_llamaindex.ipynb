{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86643700",
   "metadata": {},
   "source": [
    "# Building and Evaluating LlamaIndex CodeAct Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c356e1a",
   "metadata": {},
   "source": [
    "You can install all the dependencies for this tutorial using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08037596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-google-genai llama-index -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b243874",
   "metadata": {},
   "source": [
    "We’ll use a `.env` file to manage API keys securely. You can also set them manually as environment variables, but for this tutorial, we’ll go ahead with a `.env` setup.  \n",
    "\n",
    "Also include `.env` in your `.gitignore` to avoid accidentally exposing sensitive API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d85c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92575b4f",
   "metadata": {},
   "source": [
    "## Building CodeAct Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0143a8",
   "metadata": {},
   "source": [
    "### Defining the Tools and the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e89e5",
   "metadata": {},
   "source": [
    "First, let's configure the LLM we want to use, and provide some functions that we can use in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe1037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from google.genai import types\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    generation_config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)  # Disables thinking\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Define a few helper functions\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers together\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide two numbers\"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e1786",
   "metadata": {},
   "source": [
    "### Create a Code Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd9e3a",
   "metadata": {},
   "source": [
    "CodeAct Agent works by writing Python code (not calling tools directly) to orchestrate logic, loops, and variable management. The code executor is essential because it:\n",
    "\n",
    "- Runs the agent's generated Python code in a controlled environment\n",
    "- Maintains execution state across multiple code blocks (variables persist)  \n",
    "- Captures outputs, errors, and return values for the agent to inspect\n",
    "- Enables the agent to see results, debug issues, and continue intelligently\n",
    "\n",
    "The executor acts as the bridge between the agent's code plans and actual execution, forming the core feedback loop that makes CodeAct effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f37458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Tuple\n",
    "import io\n",
    "import contextlib\n",
    "import ast\n",
    "import traceback\n",
    "\n",
    "\n",
    "class SimpleCodeExecutor:\n",
    "    \"\"\"\n",
    "    A simple code executor that runs Python code with state persistence.\n",
    "\n",
    "    This executor maintains a global and local state between executions,\n",
    "    allowing for variables to persist across multiple code runs.\n",
    "\n",
    "    NOTE: not safe for production use! Use with caution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, locals: Dict[str, Any], globals: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize the code executor.\n",
    "\n",
    "        Args:\n",
    "            locals: Local variables to use in the execution context\n",
    "            globals: Global variables to use in the execution context\n",
    "        \"\"\"\n",
    "        # State that persists between executions\n",
    "        self.globals = globals\n",
    "        self.locals = locals\n",
    "\n",
    "    def execute(self, code: str) -> Tuple[bool, str, Any]:\n",
    "        \"\"\"\n",
    "        Execute Python code and capture output and return values.\n",
    "\n",
    "        Args:\n",
    "            code: Python code to execute\n",
    "\n",
    "        Returns:\n",
    "            Dict with keys `success`, `output`, and `return_value`\n",
    "        \"\"\"\n",
    "        # Capture stdout and stderr\n",
    "        stdout = io.StringIO()\n",
    "        stderr = io.StringIO()\n",
    "\n",
    "        output = \"\"\n",
    "        return_value = None\n",
    "        try:\n",
    "            # Execute with captured output\n",
    "            with contextlib.redirect_stdout(stdout), contextlib.redirect_stderr(stderr):\n",
    "                # Try to detect if there's a return value (last expression)\n",
    "                try:\n",
    "                    tree = ast.parse(code)\n",
    "                    last_node = tree.body[-1] if tree.body else None\n",
    "\n",
    "                    # If the last statement is an expression, capture its value\n",
    "                    if isinstance(last_node, ast.Expr):\n",
    "                        # Split code to add a return value assignment\n",
    "                        last_line = code.rstrip().split(\"\\n\")[-1]\n",
    "                        exec_code = (\n",
    "                            code[: -len(last_line)] + \"\\n__result__ = \" + last_line\n",
    "                        )\n",
    "\n",
    "                        # Execute modified code\n",
    "                        exec(exec_code, self.globals, self.locals)\n",
    "                        return_value = self.locals.get(\"__result__\")\n",
    "                    else:\n",
    "                        # Normal execution\n",
    "                        exec(code, self.globals, self.locals)\n",
    "                except:\n",
    "                    # If parsing fails, just execute the code as is\n",
    "                    exec(code, self.globals, self.locals)\n",
    "\n",
    "            # Get output\n",
    "            output = stdout.getvalue()\n",
    "            if stderr.getvalue():\n",
    "                output += \"\\n\" + stderr.getvalue()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Capture exception information\n",
    "            output = f\"Error: {type(e).__name__}: {str(e)}\\n\"\n",
    "            output += traceback.format_exc()\n",
    "\n",
    "        if return_value is not None:\n",
    "            output += \"\\n\\n\" + str(return_value)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4f7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_executor = SimpleCodeExecutor(\n",
    "    # give access to our functions defined above\n",
    "    locals={\n",
    "        \"add\": add,\n",
    "        \"subtract\": subtract,\n",
    "        \"multiply\": multiply,\n",
    "        \"divide\": divide,\n",
    "    },\n",
    "    globals={\n",
    "        # give access to all builtins\n",
    "        \"__builtins__\": __builtins__,\n",
    "        # give access to numpy\n",
    "        \"np\": __import__(\"numpy\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93316924",
   "metadata": {},
   "source": [
    "### Setup the CodeAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd03371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import CodeActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "agent = CodeActAgent(\n",
    "    code_execute_fn=code_executor.execute,\n",
    "    llm=llm,\n",
    "    tools=[add, subtract, multiply, divide],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf959c",
   "metadata": {},
   "source": [
    "### Use the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cb79e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    "    AgentStream,\n",
    ")\n",
    "\n",
    "\n",
    "async def run_agent_verbose(agent, query):\n",
    "    handler = agent.run(query)\n",
    "    print(f\"User:  {query}\")\n",
    "    async for event in handler.stream_events():\n",
    "        if isinstance(event, ToolCallResult):\n",
    "            print(f\"\\n-----------\\nCode execution result:\\n{event.tool_output}\")\n",
    "        elif isinstance(event, ToolCall):\n",
    "            print(f\"\\n-----------\\nParsed code:\\n{event.tool_kwargs['code']}\")\n",
    "        elif isinstance(event, AgentStream):\n",
    "            print(f\"{event.delta}\", end=\"\", flush=True)\n",
    "\n",
    "    return await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "937600e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Add 5 and 3, then multiply the result by 2\n",
      "<execute>\n",
      "result_add = add(5, 3)\n",
      "final_result = multiply(result_add, 2)\n",
      "print(final_result)\n",
      "</execute>\n",
      "The result is 16.\n",
      "-----------\n",
      "Parsed code:\n",
      "result_add = add(5, 3)\n",
      "final_result = multiply(result_add, 2)\n",
      "print(final_result)\n",
      "\n",
      "-----------\n",
      "Code execution result:\n",
      "16\n",
      "\n",
      "The result of adding 5 and 3, then multiplying the sum by 2 is 16."
     ]
    }
   ],
   "source": [
    "response = await run_agent_verbose(\n",
    "    agent, \"Add 5 and 3, then multiply the result by 2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6719bd2e",
   "metadata": {},
   "source": [
    "## Evaluating the LlamaIndex CodeAct Agent with Wandb weave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7e01c",
   "metadata": {},
   "source": [
    "When using Weave for evaluation, you need three main components:\n",
    "\n",
    "1. **Dataset**: A collection of queries or inputs you want to evaluate your application on.  \n",
    "\n",
    "2.\t**Model**: This is an abstraction that represents the application you want to evaluate. It’s not a literal machine learning model, but a wrapper provided by Weave that defines how your application handles input and produces output.  \n",
    "\n",
    "3. **Scorers**: These are the metrics or scoring functions that assess how well your application performs on the dataset. For example, they might check correctness, retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14fd80f",
   "metadata": {},
   "source": [
    "### Initializing the Project and Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nexus/miniconda3/envs/neo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: siddharth-plaksha.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/deep-learning-assignments/llama_index_evaluations/weave\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 📦 Published to https://wandb.ai/deep-learning-assignments/llama_index_evaluations/weave/objects/codeAct-agent-evaluation-dataset-1/versions/7VqrXTd9pa6JWTEcw6eO4Wv62UkjwGvhKK3F1zwQmyc\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave import Dataset\n",
    "\n",
    "weave.init(project_name=\"llama_index_evaluations\")\n",
    "\n",
    "eval_dataset = Dataset(\n",
    "    name=\"codeAct-agent-evaluation-dataset-1\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"id\": \"1\",\n",
    "            \"query\": \"Add 5 and 3, then multiply the result by 2\",\n",
    "            \"reference\": \"16\",\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"2\",\n",
    "            \"query\": \"Calculate the sum of the first 10 fibonacci numbers, assuming the first fibonacci number is 0\",\n",
    "            \"reference\": \"88\",\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"3\",\n",
    "            \"query\": \"Calculate the sum of all numbers from 1 to 10\",\n",
    "            \"reference\": \"55\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "weave.publish(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd9bbc",
   "metadata": {},
   "source": [
    "### Setting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3884097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "import asyncio\n",
    "from llama_index.core.agent.workflow import AgentOutput\n",
    "from llama_index.core.agent.workflow import CodeActAgent\n",
    "\n",
    "\n",
    "class LlamaIndexCodeActAgent(weave.Model):\n",
    "    @weave.op()\n",
    "    async def predict(self, query: str) -> AgentOutput:\n",
    "        agent = CodeActAgent(\n",
    "            code_execute_fn=code_executor.execute,\n",
    "            tools=[add, subtract, multiply, divide],\n",
    "            llm=llm,\n",
    "        )\n",
    "        handler = agent.run(query)\n",
    "        response = asyncio.run(handler)\n",
    "        # TODO: Look for better way to get tool description\n",
    "        return response, agent._get_tool_descriptions(tools=agent.tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382cd8c8",
   "metadata": {},
   "source": [
    "### Defining the Scorers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d099b197",
   "metadata": {},
   "source": [
    "To evaluate the performance of a CodeAct agent, we designed two LLM-based scorers focused on different aspects of correctness:  \n",
    "\n",
    "1. Tool Usage Scorer (CodeActToolUsageScorer):\n",
    "This checks whether the agent used the correct tools with valid function calls and appropriate parameter names/types. It ensures the code structure adheres to the tool definitions.  \n",
    "\n",
    "2. Task Completion Scorer (CodeActTaskCompletionScorer):\n",
    "This verifies whether the generated Python code successfully completes the task as per the user query. It considers the tool usage, execution output, and final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829f61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from textwrap import dedent\n",
    "from typing import Dict\n",
    "from weave.scorers.scorer_types import LLMScorer\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class CodeActTaskCompletionResponse(BaseModel):\n",
    "    reason: str = Field(\n",
    "        description=\"Step-by-step reasoning about whether the agent's generated code correctly completed the user's task using valid tool calls and execution\"\n",
    "    )\n",
    "    score: int = Field(\n",
    "        description=\"Binary score indicating if the task was successfully completed (1 for success, 0 for failure or incorrect tool usage)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class CodeActTaskCompletionScorer(LLMScorer):\n",
    "    name: str = \"codeact_task_completion\"\n",
    "    prompt_template: str = dedent(\n",
    "        \"\"\"\n",
    "You are evaluating a **CodeAct** agent—a type of LLM agent that generates and executes **Python code** to perform a user-specified task. CodeAct integrates with a Python interpreter, allowing the agent to call tools, run logic, self-debug, and iteratively refine its code.\n",
    "\n",
    "You are provided with:\n",
    "- A list of available **tool functions** and their signatures.\n",
    "- The **agent-generated Python code** snippet that was executed.\n",
    "- The **execution result**, including stdout or any errors.\n",
    "- The **original user query/task**.\n",
    "\n",
    "Your task:\n",
    "- Verify that the agent’s code **completes the user’s task correctly**.\n",
    "- Confirm it only uses **valid tools** with correct parameter names and types.\n",
    "- Ensure the code **executes successfully** (or handles errors intentionally and correctly).\n",
    "\n",
    "Think step by step about:\n",
    "\n",
    "1. Was the tool usage valid and correct?\n",
    "2. Did the execution output achieve the goal described in the query?\n",
    "\n",
    "Task:\n",
    "{task}\n",
    "\n",
    "Tool Definitions:\n",
    "{tool_desc}\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "Execution Output/Error:\n",
    "{execution_result}\n",
    "\n",
    "Then provide:\n",
    "\n",
    "Reasoning:\n",
    "<your detailed step-by-step reasoning>\n",
    "\n",
    "Final Score (0 or 1):\n",
    "- **1** = Task was correctly completed with valid tool usage and successful execution.\n",
    "- **0** = Task not completed, or there was invalid tool use or runtime error.\n",
    "\"\"\"\n",
    "    )\n",
    "    model_id: str = \"gemini/gemini-2.0-flash\"\n",
    "\n",
    "    @weave.op\n",
    "    async def score(self, output: tuple, query: str) -> Dict:\n",
    "        agent_output, tool_desc = output\n",
    "        # assuming the output is not multimodal\n",
    "        final_answer = agent_output.response.blocks[0].text\n",
    "\n",
    "        # extracting the python code written\n",
    "        tool_calls = agent_output.tool_calls\n",
    "        python_code = [tool_call.tool_kwargs[\"code\"] for tool_call in tool_calls]\n",
    "\n",
    "        prompt = self.prompt_template.format(\n",
    "            task=query,\n",
    "            code=\"\\n\".join(python_code),\n",
    "            tool_desc=tool_desc,\n",
    "            execution_result=final_answer,\n",
    "        )\n",
    "        response = await self._acompletion(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=CodeActTaskCompletionResponse,\n",
    "            model=self.model_id,\n",
    "        )\n",
    "        response = CodeActTaskCompletionResponse.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        )\n",
    "        return response.model_dump()\n",
    "\n",
    "\n",
    "class CodeActToolUsageResponse(BaseModel):\n",
    "    reason: str = Field(\n",
    "        description=\"Step‑by‑step reasoning about whether the CodeAct agent’s Python code correctly used the provided tools with valid function calls\"\n",
    "    )\n",
    "    score: int = Field(\n",
    "        description=\"Binary score indicating whether all tool calls were valid and correctly structured (1 for correct tool usage, 0 otherwise)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class CodeActToolUsageScorer(LLMScorer):\n",
    "    name: str = \"codeact_tool_usage_correctness\"\n",
    "    prompt_template: str = dedent(\n",
    "        \"\"\"\n",
    "You are evaluating a **CodeAct** agent, a type of LLM agent that generates and executes **Python code** to call tools directly. CodeAct agents write code snippets, run them, and can self-debug by observing results.\n",
    "\n",
    "You’re provided with:\n",
    "- A list of **available tool functions** and their signatures.\n",
    "- The **Python code** snippet the agent generated.\n",
    "\n",
    "Your task:\n",
    "- Verify the agent only calls **valid, provided tools**.\n",
    "- Check that every function call uses **correct parameter names and types**.\n",
    "\n",
    "Think step by step about each function call.\n",
    "\n",
    "Tool Definitions:\n",
    "{tool_desc}\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "Reasoning:\n",
    "<your detailed reasoning goes here>\n",
    "\n",
    "Final Verdict (0 or 1):\n",
    "- Return **1** if all tool usage is correct and runs without issues.\n",
    "- Return **0** otherwise.\n",
    "\"\"\"\n",
    "    )\n",
    "    model_id: str = \"gemini/gemini-2.0-flash\"\n",
    "\n",
    "    @weave.op\n",
    "    async def score(self, output: tuple, query: str) -> Dict:\n",
    "        agent_output, tool_desc = output\n",
    "\n",
    "        # extracting the python code written\n",
    "        tool_calls = agent_output.tool_calls\n",
    "        python_code = [tool_call.tool_kwargs[\"code\"] for tool_call in tool_calls]\n",
    "\n",
    "        prompt = self.prompt_template.format(\n",
    "            code=\"\\n\".join(python_code),\n",
    "            tool_desc=tool_desc,\n",
    "        )\n",
    "        response = await self._acompletion(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=CodeActToolUsageResponse,\n",
    "            model=self.model_id,\n",
    "        )\n",
    "        response = CodeActToolUsageResponse.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        )\n",
    "        return response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf90d36",
   "metadata": {},
   "source": [
    "Design deterministic evaluation metrics whenever possible to avoid subjective LLM judgments. Simple approaches like string matching or text embedding similarity checks can be highly effective and reliable compared to LLM-based evaluation if you know your data.\n",
    "\n",
    "In this implementation, we've structured test cases so expected answers appear directly in the agent's output. Since the agent performs deterministic calculations (like computing roots), we can evaluate correctness by simply checking if the reference answer exists in the response.\n",
    "\n",
    "While this method may not offer the highest level of flexibility, it is extremely efficient—both in terms of speed and cost—making it a practical choice for many evaluation scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ddfd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentResponseCorrectnessScorer(weave.Scorer):\n",
    "    name: str = \"agent_response_correctness\"\n",
    "\n",
    "    @weave.op\n",
    "    def score(self, output: tuple, reference: str) -> Dict:\n",
    "        agent_output, _ = output\n",
    "        final_answer = agent_output.response.blocks[0].text\n",
    "        is_present = reference.strip() in final_answer.strip()\n",
    "        return {\n",
    "            \"reason\": f\"Reference {'found' if is_present else 'not found'} in agent response.\",\n",
    "            \"score\": int(is_present),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd1e7d",
   "metadata": {},
   "source": [
    "### Performing Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8be4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_completion_scorer = CodeActTaskCompletionScorer()\n",
    "tool_usage_scorer = CodeActToolUsageScorer()\n",
    "correctness = AgentResponseCorrectnessScorer()\n",
    "\n",
    "evaluation = weave.Evaluation(\n",
    "    dataset=eval_dataset,\n",
    "    scorers=[task_completion_scorer, tool_usage_scorer, correctness],\n",
    ")\n",
    "\n",
    "llama_index_codeAct_model = LlamaIndexCodeActAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8442f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/deep-learning-assignments/llama_index_evaluations/r/call/0197dddc-ac6c-75b3-9424-b11dbb2f8b82\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 3 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 3 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 3 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"codeact_task_completion\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"codeact_tool_usage_correctness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"agent_response_correctness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 3.2990194161732993\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'codeact_task_completion': {'score': {'mean': 1.0}},\n",
       " 'codeact_tool_usage_correctness': {'score': {'mean': 1.0}},\n",
       " 'agent_response_correctness': {'score': {'mean': 1.0}},\n",
       " 'model_latency': {'mean': 3.2990194161732993}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "asyncio.run(evaluation.evaluate(llama_index_codeAct_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
