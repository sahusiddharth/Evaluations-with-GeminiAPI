{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3994d113",
   "metadata": {},
   "source": [
    "# Building and Evaluating LlamaIndex ReAct Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b238c85",
   "metadata": {},
   "source": [
    "You can install all the dependencies for this tutorial using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install litellm llama-index-embeddings-google-genai llama-index-llms-google-genai llama-index weave -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8217f0a",
   "metadata": {},
   "source": [
    "We‚Äôll use a `.env` file to manage API keys securely. You can also set them manually as environment variables, but for this tutorial, we‚Äôll go ahead with a `.env` setup.  \n",
    "\n",
    "Also include `.env` in your `.gitignore` to avoid accidentally exposing sensitive API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd511cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8979ddf",
   "metadata": {},
   "source": [
    "## Building ReAct Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d83cbb",
   "metadata": {},
   "source": [
    "ReAct breaks down complex tasks into a series of thoughts, actions, and observations, ReAct agents can tackle intricate problems with a level of transparency and adaptability that was previously challenging to achieve. This methodology allows for a more nuanced understanding of the agent‚Äôs decision-making process, making it easier for developers to debug, refine, and optimize LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1626dd0",
   "metadata": {},
   "source": [
    "### Defining the Tools and the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f63501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from google.genai import types\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    generation_config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001eb40f",
   "metadata": {},
   "source": [
    "### Setting up the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0a3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "\n",
    "\n",
    "agent = ReActAgent(tools=[multiply, add], llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20a082",
   "metadata": {},
   "source": [
    "### Try It Out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0142fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "Thought: The current language of the user is: English. I need to perform the calculation 20 + (2 * 4). I will start by multiplying 2 and 4, and then adding the result to 20.\n",
      "Action: multiply\n",
      "Action Input: {\"a\": 2, \"b\": 4}\n",
      "```Thought: The current language of the user is: English. Now I need to add 8 to 20.\n",
      "Action: add\n",
      "Action Input: {'a': 20, 'b': 8}Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: 28\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentStream\n",
    "\n",
    "handler = agent.run(\"What is 20+(2*4)?\")\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, AgentStream):\n",
    "        print(f\"{ev.delta}\", end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dfbdc3",
   "metadata": {},
   "source": [
    "## Evaluating the Agent with Wandb weave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8d965",
   "metadata": {},
   "source": [
    "When using Weave for evaluation, you need three main components:\n",
    "\n",
    "1. **Dataset**: A collection of queries or inputs you want to evaluate your application on.  \n",
    "\n",
    "2.\t**Model**: This is an abstraction that represents the application you want to evaluate. It‚Äôs not a literal machine learning model, but a wrapper provided by Weave that defines how your application handles input and produces output.  \n",
    "\n",
    "3. **Scorers**: These are the metrics or scoring functions that assess how well your application performs on the dataset. For example, they might check correctness, retrieval quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a211a2c",
   "metadata": {},
   "source": [
    "### Initializing the Project and Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814dd42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nexus/miniconda3/envs/gsoc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: siddharth-plaksha.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/deep-learning-assignments/llama_index_react_agent_evaluations/weave\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: üì¶ Published to https://wandb.ai/deep-learning-assignments/llama_index_react_agent_evaluations/weave/objects/react-agent-evaluation-dataset/versions/85yI6eRFBYkFCHp3ZgKLg1G0PTEsU2wXlXkgr5cBq3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectRef(entity='deep-learning-assignments', project='llama_index_react_agent_evaluations', name='react-agent-evaluation-dataset', _digest='85yI6eRFBYkFCHp3ZgKLg1G0PTEsU2wXlXkgr5cBq3s', _extra=())"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/deep-learning-assignments/llama_index_react_agent_evaluations/r/call/0197e2f4-613f-7316-91fa-88c66e3dcc0a\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from weave import Dataset\n",
    "\n",
    "weave.init(project_name=\"llama_index_react_agent_evaluations\")\n",
    "\n",
    "dataset = Dataset(\n",
    "    name=\"react-agent-evaluation-dataset\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"id\": \"0\",\n",
    "            \"query\": \"What is 5+3+2\",\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"0\",\n",
    "            \"query\": \"What is 20+(2*4)?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61656687",
   "metadata": {},
   "source": [
    "### Setting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc79a424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from typing import Sequence, List\n",
    "from llama_index.core.agent.workflow import (\n",
    "    AgentOutput,\n",
    "    AgentStream,\n",
    ")\n",
    "from llama_index.core.tools import BaseTool\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "\n",
    "react_system_header_str = \"\"\"\\\n",
    "\n",
    "You are designed to help with a variety of tasks, from answering questions \\\n",
    "    to providing summaries to other types of analyses.\n",
    "\n",
    "## Tools\n",
    "You have access to a wide variety of tools. You are responsible for using\n",
    "the tools in any sequence you deem appropriate to complete the task at hand.\n",
    "This may require breaking the task into subtasks and using different tools\n",
    "to complete each subtask.\n",
    "\n",
    "You have access to the following tools:\n",
    "{tool_desc}\n",
    "\n",
    "## Output Format\n",
    "To answer the question, please use the following format.\n",
    "\n",
    "```\n",
    "Thought: One-liner explanation for the tool selection and parameter selection\n",
    "Action: tool name (one of {tool_names}) if using a tool.\n",
    "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
    "```\n",
    "\n",
    "Please ALWAYS start with a Thought.\n",
    "\n",
    "Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
    "\n",
    "If this format is used, the user will respond in the following format:\n",
    "\n",
    "```\n",
    "Observation: tool response\n",
    "```\n",
    "\n",
    "You should keep repeating the above format until you have enough information\n",
    "to answer the question without using any more tools. At that point, you MUST respond\n",
    "in the one of the following two formats:\n",
    "\n",
    "```\n",
    "Thought: your one-liner thought here, stating how you have everything to complete the task\n",
    "Answer: [your answer here]\n",
    "```\n",
    "\n",
    "```\n",
    "Thought: I cannot answer the question with the provided tools.\n",
    "Answer: Sorry, I cannot answer your query.\n",
    "```\n",
    "\n",
    "## Additional Rules\n",
    "- The answer MUST contain a sequence of bullet points that explain how you arrived at the answer. This can include aspects of the previous conversation history.\n",
    "- You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
    "\n",
    "## Current Conversation\n",
    "Below is the current conversation consisting of interleaving human and assistant messages.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "react_system_prompt = PromptTemplate(react_system_header_str)\n",
    "\n",
    "llm = GoogleGenAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "\n",
    "class LlamaIndexReActAgent(weave.Model):\n",
    "    @staticmethod\n",
    "    def get_react_tool_descriptions(tools: Sequence[BaseTool]) -> List[str]:\n",
    "        \"\"\"Tool.\"\"\"\n",
    "        tool_descs = []\n",
    "        for tool in tools:\n",
    "            tool_desc = (\n",
    "                f\"> Tool Name: {tool.metadata.name}\\n\"\n",
    "                f\"Tool Description: {tool.metadata.description}\\n\"\n",
    "                f\"Tool Args: {tool.metadata.fn_schema_str}\\n\"\n",
    "            )\n",
    "            tool_descs.append(tool_desc)\n",
    "        return tool_descs\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, query: str) -> AgentOutput:\n",
    "\n",
    "        agent = ReActAgent(tools=[multiply, add], llm=llm)\n",
    "        agent.update_prompts({\"react_header\": react_system_prompt})\n",
    "\n",
    "        handler = agent.run(query)\n",
    "        trace = []\n",
    "        async for ev in handler.stream_events():\n",
    "            if not isinstance(ev, AgentStream):\n",
    "                trace.append(ev)\n",
    "\n",
    "        response = await handler\n",
    "        return trace, self.get_react_tool_descriptions(agent.tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7e6a2",
   "metadata": {},
   "source": [
    "### Defining the Scorers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d453b4",
   "metadata": {},
   "source": [
    "To evaluate agent tool usage of a ReAct agent, we designed three LLM-based scorers focused on different aspects:\n",
    "\n",
    "- **Parameter Selection:** Measures the agent's accuracy in providing correct parameters when invoking selected tools. It evaluates whether all required parameters are included, parameter names and types match the tool signature, and no invalid or extraneous parameters are passed.\n",
    "\n",
    "- **Tool Selection:** Measures the agent's ability to identify and select the most appropriate tool(s) for accomplishing the given task. It evaluates whether the chosen tool matches the task requirements, if tools were used when necessary, and if irrelevant tools were avoided.\n",
    "\n",
    "- **Tool Calling:** Measures the complete correctness of tool invocation by combining both tool selection and parameter accuracy. It evaluates whether the agent made the right tool call with correct parameters, representing end-to-end tool usage effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914dee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from pydantic import BaseModel, Field\n",
    "import weave\n",
    "from weave.scorers.scorer_types import LLMScorer\n",
    "from typing import Dict\n",
    "from llama_index.core.agent.workflow import AgentInput\n",
    "\n",
    "\n",
    "class ParameterSelectionCorrectnessResponse(BaseModel):\n",
    "    score: float = Field(\n",
    "        description=dedent(\n",
    "            \"\"\"\n",
    "        A float score indicating correctness of tool parameter usage:\n",
    "        - 1.0 = All tool calls were valid and used correct parameters.\n",
    "        - 0.0 = No tool calls were made by the assistant.\n",
    "        - -1.0 = At least one tool call was made, but with missing, incorrect, or invalid parameters.\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "    )\n",
    "\n",
    "\n",
    "class ParameterSelectionCorrectnessScorer(LLMScorer):\n",
    "    name: str = \"parameter_selection_correctness\"\n",
    "    prompt_template: str = dedent(\n",
    "        \"\"\"\n",
    "        You are designed to **evaluate whether tools were correctly used** in a conversation between an assistant and a user.\n",
    "\n",
    "        ## Tools\n",
    "        You have access to a list of tools and their function signatures. Your task is NOT to call them, but to inspect whether the assistant **previously called these tools correctly**, based on the function signature.\n",
    "\n",
    "        You must check:\n",
    "        - Whether the assistant used only tools listed in the available tools.\n",
    "        - Whether required parameters were correctly included.\n",
    "        - Whether no extra or invalid parameters were used.\n",
    "        - Whether parameters follow the correct types or structure.\n",
    "\n",
    "        ## Output Format\n",
    "        You must respond with **only one** of the following three values:\n",
    "        - \"1.0\" ‚Üí All tool calls were valid and used correct parameters.\n",
    "        - \"0.0\" ‚Üí No tool calls were made by the assistant.\n",
    "        - \"-1.0\" ‚Üí At least one tool call was made, but with missing, incorrect, or invalid parameters.\n",
    "\n",
    "        Do not include any other explanation, justification, or text in your output.\n",
    "\n",
    "        ## Tool Definitions\n",
    "        Here are the available tools and their parameters:\n",
    "        {tool_desc}\n",
    "\n",
    "        ## Trace\n",
    "        Below is the trace with possible tool calls. Tool calls are in the format:\n",
    "        CALL: tool_name(param1=value1, param2=value2)\n",
    "\n",
    "        [BEGIN TRACE]\n",
    "        {trace}\n",
    "        [END TRACE]\n",
    "\n",
    "        Evaluate the assistant's tool usage in this trace based on the tool definitions above, and output only the correct value from the list: \"1.0\", \"0.0\", \"-1.0\".\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    model_id: str = \"gemini/gemini-2.0-flash\"\n",
    "\n",
    "    @weave.op\n",
    "    async def score(self, output: tuple, query: str) -> Dict:\n",
    "        # output contains trace and tool descriptions\n",
    "        trace, tool_desc = output\n",
    "\n",
    "        for event in trace:\n",
    "            if isinstance(event, AgentInput):\n",
    "                agent_input = event\n",
    "\n",
    "        trace = \"\\n\".join([i.__str__() for i in agent_input.input[1:]])\n",
    "\n",
    "        prompt = self.prompt_template.format(trace=trace, tool_desc=tool_desc)\n",
    "\n",
    "        response = await self._acompletion(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=ParameterSelectionCorrectnessResponse,\n",
    "            model=self.model_id,\n",
    "        )\n",
    "        parsed = ParameterSelectionCorrectnessResponse.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        )\n",
    "        return parsed.model_dump()\n",
    "\n",
    "\n",
    "class ToolSelectionCorrectnessResponse(BaseModel):\n",
    "    score: float = Field(\n",
    "        description=dedent(\n",
    "            \"\"\"\n",
    "        A float score indicating correctness of tool selection:\n",
    "        - 1.0 = The assistant selected and used the most appropriate tools for the user‚Äôs task.\n",
    "        - 0.0 = No tools were selected by the assistant, even though tool use was clearly necessary.\n",
    "        - -1.0 = The assistant selected or used tools that were clearly incorrect or suboptimal for the user‚Äôs task.\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "    )\n",
    "\n",
    "\n",
    "class ToolSelectionCorrectnessScorer(LLMScorer):\n",
    "    name: str = \"tool_selection_correctness\"\n",
    "    prompt_template: str = dedent(\n",
    "        \"\"\"\n",
    "        You are designed to **evaluate whether the correct tools were selected and used** in a conversation between an assistant and a user.\n",
    "\n",
    "        ## Task\n",
    "        You are provided with a list of tools and their descriptions. Your goal is NOT to execute or validate tool parameters, but to assess whether the assistant chose the **most appropriate tool(s)** for the user's intent, based on the tool capabilities.\n",
    "\n",
    "        You must check:\n",
    "        - Whether the assistant selected tools relevant to the user‚Äôs request.\n",
    "        - Whether any obviously incorrect or irrelevant tools were used.\n",
    "        - Whether tool usage was necessary for the given task.\n",
    "        - Whether the assistant missed calling a necessary tool when it should have.\n",
    "\n",
    "        ## Output Format\n",
    "        You must respond with **only one** of the following three values:\n",
    "        - \"1.0\" ‚Üí The assistant selected and used the most appropriate tools for the user‚Äôs task.\n",
    "        - \"0.0\" ‚Üí No tools were selected by the assistant, even though tool use was clearly necessary.\n",
    "        - \"-1.0\" ‚Üí The assistant selected or used tools that were clearly incorrect or suboptimal for the user‚Äôs task.\n",
    "\n",
    "        Do not include any other explanation, justification, or text in your output.\n",
    "\n",
    "        ## Tool Descriptions\n",
    "        Here are the available tools and what they are used for:\n",
    "\n",
    "        {tool_desc}\n",
    "\n",
    "        ## Trace\n",
    "        Below is the trace along with the reasoning of selecting tools with possible tool calls. Tool calls are in the format:\n",
    "        CALL: tool_name(param1=value1, param2=value2)\n",
    "\n",
    "        [BEGIN TRACE]\n",
    "        {trace}\n",
    "        [END TRACE]\n",
    "\n",
    "        Evaluate the assistant‚Äôs **tool selection** based on the user's needs and the tool descriptions above, and output only the correct value from the list: \"1.0\", \"0.0\", \"-1.0\".\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    model_id: str = \"gemini/gemini-2.0-flash\"\n",
    "\n",
    "    @weave.op\n",
    "    async def score(self, output: tuple, query: str) -> Dict:\n",
    "        trace, tool_desc = output\n",
    "\n",
    "        prompt = self.prompt_template.format(trace=trace, tool_desc=tool_desc)\n",
    "\n",
    "        response = await self._acompletion(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=ToolSelectionCorrectnessResponse,\n",
    "            model=self.model_id,\n",
    "        )\n",
    "        parsed = ToolSelectionCorrectnessResponse.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        )\n",
    "        return parsed.model_dump()\n",
    "\n",
    "\n",
    "class ToolAccuracyResponse(BaseModel):\n",
    "    score: float = Field(\n",
    "        description=dedent(\n",
    "            \"\"\"\n",
    "        A float score indicating accuracy of tool usage:\n",
    "        - 1.0 = The assistant used the correct tool(s) at the appropriate time and passed all parameters correctly.\n",
    "        - 0.0 = No tool was used by the assistant, even though tool usage was clearly necessary.\n",
    "        - -1.0 = A tool was used, but either the wrong tool was selected or it was invoked with missing, incorrect, or invalid parameters.\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "    )\n",
    "\n",
    "\n",
    "class ToolAccuracyScorer(LLMScorer):\n",
    "    name: str = \"tool_accuracy\"\n",
    "    prompt_template: str = dedent(\n",
    "        \"\"\"\n",
    "        You are designed to **evaluate whether the assistant accurately used tools** in a conversation with a user.\n",
    "\n",
    "        ## Task\n",
    "        You are provided with a list of tools, their descriptions, and function signatures. Your job is to inspect the assistant‚Äôs tool usage in a conversation trace. Your evaluation must consider **both**:\n",
    "        - Whether the assistant selected the **correct tool(s)** to fulfill the user's intent.\n",
    "        - Whether the tool(s) were invoked with the **correct parameters**, based on the function signatures.\n",
    "\n",
    "        You must check:\n",
    "        - Did the assistant use tools relevant to the user's task?\n",
    "        - Were any necessary tools **omitted**?\n",
    "        - Were any tools used that were **clearly irrelevant**?\n",
    "        - Did the assistant use only tools listed in the available tools?\n",
    "        - Were all **required parameters included**?\n",
    "        - Were there any **extra or invalid parameters**?\n",
    "        - Did the parameters follow the **correct names, types, and structure**?\n",
    "\n",
    "        ## Output Format\n",
    "        You must respond with **only one** of the following three values:\n",
    "        - \"1.0\" ‚Üí The assistant used the correct tool(s) at the appropriate time and passed all parameters correctly.\n",
    "        - \"0.0\" ‚Üí No tool was used by the assistant, even though tool usage was clearly necessary.\n",
    "        - \"-1.0\" ‚Üí A tool was used, but either the wrong tool was selected or it was invoked with missing, incorrect, or invalid parameters.\n",
    "\n",
    "        Do not include any other explanation, justification, or text in your output.\n",
    "\n",
    "        ## Tool Definitions\n",
    "        Here are the available tools and their function signatures:\n",
    "\n",
    "        {tool_desc}\n",
    "\n",
    "        ## Trace\n",
    "        Below is the trace reasoning of of tool call selection. Tool calls are in the format:\n",
    "        CALL: tool_name(param1=value1, param2=value2)\n",
    "\n",
    "        [BEGIN TRACE]\n",
    "        {trace}\n",
    "        [END TRACE]\n",
    "\n",
    "        Evaluate the assistant‚Äôs tool usage based on the above definitions, and output only the correct value from the list: \"1.0\", \"0.0\", \"-1.0\".\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    model_id: str = \"gemini/gemini-2.0-flash\"\n",
    "\n",
    "    @weave.op\n",
    "    async def score(self, output: tuple, query: str) -> Dict:\n",
    "        trace, tool_desc = output\n",
    "\n",
    "        prompt = self.prompt_template.format(trace=trace, tool_desc=tool_desc)\n",
    "\n",
    "        response = await self._acompletion(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format=ToolAccuracyResponse,\n",
    "            model=self.model_id,\n",
    "        )\n",
    "        parsed = ToolAccuracyResponse.model_validate_json(\n",
    "            response.choices[0].message.content\n",
    "        )\n",
    "        return parsed.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d2af4",
   "metadata": {},
   "source": [
    "### Performing Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f8e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_selection_scorer = ParameterSelectionCorrectnessScorer()\n",
    "tool_selection_scorer = ToolSelectionCorrectnessScorer()\n",
    "tool_accuracy = ToolAccuracyScorer()\n",
    "\n",
    "evaluation = weave.Evaluation(\n",
    "    dataset=dataset,\n",
    "    scorers=[parameter_selection_scorer, tool_selection_scorer, tool_accuracy],\n",
    ")\n",
    "\n",
    "llama_index_reAct_model = LlamaIndexReActAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82839c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 2 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 2 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"parameter_selection_correctness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"tool_selection_correctness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"tool_accuracy\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"score\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 4.0369240045547485\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'parameter_selection_correctness': {'score': {'mean': 1.0}},\n",
       " 'tool_selection_correctness': {'score': {'mean': 1.0}},\n",
       " 'tool_accuracy': {'score': {'mean': 1.0}},\n",
       " 'model_latency': {'mean': 4.0369240045547485}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "asyncio.run(evaluation.evaluate(llama_index_reAct_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
